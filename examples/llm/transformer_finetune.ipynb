{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adammoss/MLiS2/blob/master/examples/llm/transformer_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeUfbxv9ghmQ"
      },
      "source": [
        "As an example application of a transformer, let's build a GPT like language model that predicts the probability of a sentence of $\\tau$ tokens,\n",
        "\n",
        "$\n",
        "P \\left( \\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}, \\ldots, \\boldsymbol{x}^{(\\tau)}    \\right) = \\prod_{t  = 1}^{\\tau} P \\left( \\boldsymbol{x}^{(i)} |  \\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(i-1)}    \\right)\n",
        "$\n",
        "\n",
        "where $\\boldsymbol{x}^{(t)}$ is a vector representing a token.\n",
        "\n",
        "The script uses the 'tiny_shakespeare' dataset, but it's designed to work with other text sources as well. It showcases how to prepare datasets for training and testing, configure model parameters like batch size and context size, and fine-tune a pre-trained GPT-2 model for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48zoWwDaisC6",
        "outputId": "3022d622-1d27-4b4a-d7cf-89e044357127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (23.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2023.12.25)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (13.7.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (0.1.9)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_nlp) (4.66.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_nlp) (2.16.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text->keras_nlp) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_nlp) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text->keras_nlp) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_nlp\n",
        "!pip install tensorflow_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "Alq1W_bCWoTM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import os\n",
        "import requests\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "60FGqR6mHMjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab08ba95-162c-4230-acf1-aa6c0d7f48d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "wNrbkRtsQOfJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "context_size = 256\n",
        "#docs = ['_chat.txt']\n",
        "docs = ['tiny_shakespeare']\n",
        "#docs = ['scientific_papers/arxiv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "NfpGBFRj-DzH"
      },
      "outputs": [],
      "source": [
        "train_text = ''\n",
        "test_text = ''\n",
        "for doc in docs:\n",
        "  if doc == 'tiny_shakespeare':\n",
        "    d = tfds.load(name=doc)['train']\n",
        "    train_text += next(iter(d))['text'].numpy().decode(\"utf-8\")\n",
        "    d = tfds.load(name=doc)['test']\n",
        "    test_text += next(iter(d))['text'].numpy().decode(\"utf-8\")\n",
        "  elif doc == 'scientific_papers/arxiv':\n",
        "    d = tfds.load(name=doc)\n",
        "  else:\n",
        "    if not os.path.isfile(doc):\n",
        "      from google.colab import files\n",
        "      uploaded = files.upload()\n",
        "    sentences = []\n",
        "    with open(doc, 'r') as f:\n",
        "      for x in f.readlines():\n",
        "        if 'omitted' not in x:\n",
        "          if len(x.split(']')) > 1:\n",
        "            sentences.append(x.split(']')[1])\n",
        "          else:\n",
        "            sentences.append(x)\n",
        "    text = ''.join(sentences)\n",
        "    train_text += text[:int(0.8*len(text))]\n",
        "    test_text += text[int(0.8*len(text)):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "dxTX6JzJBSoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afeffa6c-163f-4ae1-f6b4-8a8cd1ee189f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "print(train_text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_text.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igor84fAmUIh",
        "outputId": "7689b16f-23f1-462e-ab4d-336b32706c88"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153276"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "0mZHQaprdjaU"
      },
      "outputs": [],
      "source": [
        "split_train_ds = tf.data.Dataset.from_tensor_slices(tf.strings.split(train_text, sep=' '))\n",
        "split_test_ds = tf.data.Dataset.from_tensor_slices(tf.strings.split(test_text, sep=' '))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "G6O0NGlwfHWj"
      },
      "outputs": [],
      "source": [
        "train_sequences = split_train_ds.batch(50, drop_remainder=True)\n",
        "test_sequences = split_test_ds.batch(50, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "P08FJYMUfXWh"
      },
      "outputs": [],
      "source": [
        "def join_input(sequence):\n",
        "  return tf.strings.reduce_join(sequence, axis=-1, separator=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "ShqXjIxBhr9w"
      },
      "outputs": [],
      "source": [
        "train_ds = train_sequences.map(join_input)\n",
        "test_ds = test_sequences.map(join_input)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(batch_size, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "test_ds = (\n",
        "    test_ds\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "Iu79sruGorjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2774c0a3-d041-4203-b48f-984f9c24e170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"solicit him\\nFor mercy to his country. Therefore, let's hence,\\nAnd with our fair entreaties haste them on.\\n\\nFirst Senator:\\nStay: whence are you?\\n\\nSecond Senator:\\nStand, and go back.\\n\\nMENENIUS:\\nYou guard like men; 'tis well: but, by your leave,\\nI am an officer of state, and come\\nTo speak with Coriolanus.\\n\\nFirst Senator:\\nFrom whence?\\n\\nMENENIUS:\\nFrom Rome.\\n\\nFirst Senator:\\nYou may not pass,\"\n"
          ]
        }
      ],
      "source": [
        "x = next(iter(train_ds))\n",
        "print(x.numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = next(iter(test_ds))\n",
        "print(x.numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_dQYDPu2wB_",
        "outputId": "55a1f87a-eae5-4112-c45a-367bf0ddf7e7"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"rance ta'en\\nAs shall with either part's agreement stand?\\n\\nBAPTISTA:\\nNot in my house, Lucentio; for, you know,\\nPitchers have ears, and I have many servants:\\nBesides, old Gremio is hearkening still;\\nAnd happily we might be interrupted.\\n\\nTRANIO:\\nThen at my lodging, an it like you:\\nThere doth my father lie; and there, this night,\\nWe'll pass the business\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "VyWGnmO0yl67"
      },
      "outputs": [],
      "source": [
        "# To speed up training and generation, we do not use a the full GPT2 context length of 1024.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=context_size,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "1Gi52RaSuGe2",
        "outputId": "37fab941-4c62-45f1-a4d0-fa30e839c9c8"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gpt2_causal_lm_preprocessor_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gpt2_causal_lm_preprocessor_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (\u001b[38;5;33mGPT2Tokenizer\u001b[0m)                     │                                              \u001b[38;5;34m50,257\u001b[0m │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gpt2_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Tokenizer</span>)                     │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">50,257</span> │\n",
              "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gpt2_causal_lm_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_causal_lm_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (\u001b[38;5;33mGPT2Backbone\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │     \u001b[38;5;34m124,439,808\u001b[0m │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)                    │      \u001b[38;5;34m38,597,376\u001b[0m │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ gpt2_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2Backbone</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> │\n",
              "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
              "│ token_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)                    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,597,376</span> │\n",
              "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,439,808\u001b[0m (474.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,439,808</span> (474.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "j7dJWes5gUjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46057eee-1c4c-494e-f871-3787d6fc2dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "That Italian restaurant is now open to the public.\n",
            "\n",
            "The restaurant was originally slated to open at 7 p.m., but has been closed due to a fire. The owners of the restaurant have since decided to open at 7 p.m. The fire is believed to have been caused by the fire.\n",
            "\n",
            "\"We have been working with the fire department and the local authorities to make sure we get the restaurant back up to the normal schedule,\" said chef Giovanni Graziano.\n",
            "\n",
            "The restaurant was originally scheduled to open at 6 p.m. on Saturday. The fire department was called to the restaurant because the owner was injured in an explosion that was reported at the restaurant.\n",
            "\n",
            "The fire department was able to get the fire out of the restaurant and to the fire station, where firefighters found the fire. Graziano was not able to confirm or deny the fire.\n",
            "\n",
            "The fire is still under investigation by the fire marshal's department and the fire station's\n",
            "TOTAL TIME ELAPSED: 15.17s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "dqhpkdQ-hK0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c6421e-49f4-437d-efa1-605854fbcc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "47/47 [==============================] - 72s 558ms/step - loss: 1.5679 - accuracy: 0.3093 - val_loss: 1.6007 - val_accuracy: 0.3477\n",
            "Epoch 2/10\n",
            "47/47 [==============================] - 16s 342ms/step - loss: 1.4351 - accuracy: 0.3434 - val_loss: 1.5743 - val_accuracy: 0.3592\n",
            "Epoch 3/10\n",
            "47/47 [==============================] - 16s 341ms/step - loss: 1.3903 - accuracy: 0.3552 - val_loss: 1.5646 - val_accuracy: 0.3648\n",
            "Epoch 4/10\n",
            "47/47 [==============================] - 16s 340ms/step - loss: 1.3644 - accuracy: 0.3623 - val_loss: 1.5606 - val_accuracy: 0.3689\n",
            "Epoch 5/10\n",
            "47/47 [==============================] - 16s 340ms/step - loss: 1.3455 - accuracy: 0.3675 - val_loss: 1.5566 - val_accuracy: 0.3695\n",
            "Epoch 6/10\n",
            "47/47 [==============================] - 16s 341ms/step - loss: 1.3296 - accuracy: 0.3723 - val_loss: 1.5546 - val_accuracy: 0.3707\n",
            "Epoch 7/10\n",
            "47/47 [==============================] - 16s 339ms/step - loss: 1.3177 - accuracy: 0.3757 - val_loss: 1.5547 - val_accuracy: 0.3736\n",
            "Epoch 8/10\n",
            "47/47 [==============================] - 16s 341ms/step - loss: 1.3083 - accuracy: 0.3793 - val_loss: 1.5544 - val_accuracy: 0.3729\n",
            "Epoch 9/10\n",
            "47/47 [==============================] - 18s 383ms/step - loss: 1.3023 - accuracy: 0.3803 - val_loss: 1.5544 - val_accuracy: 0.3729\n",
            "Epoch 10/10\n",
            "47/47 [==============================] - 16s 341ms/step - loss: 1.2976 - accuracy: 0.3814 - val_loss: 1.5544 - val_accuracy: 0.3733\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78bfb8a675b0>"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "# Linearly decaying learning rate.\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs, validation_data=test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "qAbp0DHNivVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6689b0b-8ed6-4c85-bc27-6e8a83ad7f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TIME ELAPSED: 13.94s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "conversation = \"A\"\n",
        "\n",
        "for _ in range(1):\n",
        "  input = ' '.join(conversation.split()[-30:])\n",
        "  output = gpt2_lm.generate(input, max_length=200)\n",
        "  conversation += output[len(input):]\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "BqQBat1qjVO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b145090b-2160-43c7-fc3c-e44a7d791fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man\n",
            "To whom I would not give aught.\n",
            "But what I will do,\n",
            "I'll not say, 'tis the most noble deed\n",
            "Which I have done.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why, what?\n",
            "The most noble deed?\n",
            "\n",
            "KING RICHARD II:\n",
            "I would not give it, for I would\n",
            "Have done it in the most noble way\n",
            "Which is most fair.\n",
            "Why, what?\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not give it for\n"
          ]
        }
      ],
      "source": [
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIBOoEp74AtM"
      },
      "execution_count": 221,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNZmTlQntezwX23vrUSjKx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}